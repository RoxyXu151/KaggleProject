{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32ef6f96",
   "metadata": {},
   "source": [
    "1. ID3决策树：\n",
    "    1. 算法: 从根节点开始，每次选取使信息增益$I(X,Y)$最大的特征进行分类，并对产生的子节点递归进行特征选取和分裂，知道所有节点上的数据都属于同一类为止。\n",
    "    2. 缺点：如果特征本身较为复杂(例如编号，有序等)，则可能导致过拟合。并且ID3算法直到所有特征都被使用完毕才会停止分裂，这可能导致树的深度过大，影响模型的泛化能力。\n",
    "2. C4.5决策树：改用信息增益率$R(X,Y)$来选择特征进行分裂，避免了ID3算法的过拟合问题。\n",
    "    1. 算法: 从根节点开始，每次选取使信息增益率$R(X,Y)$最大的特征进行分类，并对产生的子节点递归进行特征选取和分裂，直到所有节点上的数据都属于同一类为止。\n",
    "    2. 优点：通过使用信息增益率来选择特征，C4.5算法能够更好地处理连续特征和缺失值，并且可以生成更小的决策树。\n",
    "3. 代价函数：$C(T)=\\sum\\limits_{t=1}^{|T|}N_tH_t(T)+\\lambda|T|$, 其中$H_t(T)=-\\sum\\limits_{k}\\frac{N_{tk}}{N_t}log\\frac{N_{tk}}{N_t}$, $N_t$为节点$t$上的样本数，$N_{tk}$为节点$t$上第$k$类样本的数目，$\\lambda$为正则化参数。\n",
    "    1. 代价函数的第一项是所有节点的熵，第二项是树的复杂度。\n",
    "    2. 通过最小化代价函数，可以得到最优的决策树结构。\n",
    "4. CART决策树：CART（Classification and Regression Trees）算法由特征选择，树的生成及剪枝组成，既可以分类也可以回归，统称为决策树\n",
    "    1. 分类树：\n",
    "        0. 基尼指数：$Gini(P)=\\sum\\limits_{k=1}^{K}p_k(1-p_k)$, 其中$p_k$为节点$t$上第$k$类样本的比例。基尼指数越大，样本集合的不确定性越大。\n",
    "        1. 树的生成：\n",
    "            1. 设节点的训练数据集为D，计算现有特征对该数据集的基尼指数，对每一个特征A，对其可能取的每个值a，根据样本点对A=a的取值与否，划分为D_1,D_2两部分，根据$Gini(D,A)=\\frac{|D_1|}{|D|}Gini(D_1)+\\frac{|D_2|}{|D|}Gini(D_2)$计算**在特征A是否取a的条件下，集合D的基尼指数**\n",
    "            2. 选择基尼指数最小的**特征A和对应切分点a作为最优特征和最优切分点**，将D划分为D_1,D_2两部分\n",
    "            3. 对D_1,D_2两部分递归进行特征选择和划分，直到满足**停止条件**\n",
    "            4. 返回CART决策树\n",
    "        2.  树的剪枝：每次选择最小的alpha也就是减去最小的枝\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e67d4de",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf750116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n",
      "None\n",
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "\n",
      "                                                Name     Sex   Age  SibSp  \\\n",
      "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "4                           Allen, Mr. William Henry    male  35.0      0   \n",
      "\n",
      "   Parch            Ticket     Fare Cabin Embarked  \n",
      "0      0         A/5 21171   7.2500   NaN        S  \n",
      "1      0          PC 17599  71.2833   C85        C  \n",
      "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3      0            113803  53.1000  C123        S  \n",
      "4      0            373450   8.0500   NaN        S  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# 读取数据\n",
    "data=pd.read_csv(\"data/train.csv\")\n",
    "print(data.info())\n",
    "print(data[:5])\n",
    "\n",
    "# 删去编号、姓名、船票编号3列, inplace表示在原数据上修改\n",
    "data.drop(columns=['PassengerId','Name','Ticket'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f94e4a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age :\n",
      "0.4200\n",
      "9.2622\n",
      "18.1044\n",
      "26.9467\n",
      "35.7889\n",
      "44.6311\n",
      "53.4733\n",
      "62.3156\n",
      "71.1578\n",
      "80.0000\n",
      "Fare :\n",
      "0.0000\n",
      "56.9255\n",
      "113.8509\n",
      "170.7764\n",
      "227.7019\n",
      "284.6273\n",
      "341.5528\n",
      "398.4783\n",
      "455.4037\n",
      "512.3292\n"
     ]
    }
   ],
   "source": [
    "feat_ranges={}\n",
    "cont_feat=['Age', 'Fare'] # 连续特征\n",
    "bins=10 # 连续特征分箱数,即分成多少个区间\n",
    "\n",
    "for feat in cont_feat:\n",
    "    # 数据集中存在缺省值nan，需要用np.nanmin()和np.nanmax()来计算\n",
    "    min_val=np.nanmin(data[feat])# np.nanmin()计算忽略nan的最小值\n",
    "    max_val=np.nanmax(data[feat])# np.nanmax()计算忽略nan的最大值\n",
    "    feat_ranges[feat]=np.linspace(min_val, max_val, bins).tolist() # 生成分箱边界\n",
    "    print(feat,\":\") # 查看分类点\n",
    "    for spt in feat_ranges[feat]:\n",
    "        print(f'{spt:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc64adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sex:Index(['female', 'male'], dtype='object')\n",
      "Sex编码后： [1 0]\n",
      "Pclass:Index([1, 2, 3], dtype='int64')\n",
      "Pclass编码后： [2 0 1]\n",
      "SibSp:Index([0, 1, 2, 3, 4, 5, 8], dtype='int64')\n",
      "SibSp编码后： [1 0 3 4 2 5 6]\n",
      "Parch:Index([0, 1, 2, 3, 4, 5, 6], dtype='int64')\n",
      "Parch编码后： [0 1 2 5 3 4 6]\n",
      "Cabin:Index(['A10', 'A14', 'A16', 'A19', 'A20', 'A23', 'A24', 'A26', 'A31', 'A32',\n",
      "       ...\n",
      "       'E8', 'F E69', 'F G63', 'F G73', 'F2', 'F33', 'F38', 'F4', 'G6', 'T'],\n",
      "      dtype='object', length=147)\n",
      "Cabin编码后： [ -1  81  55 129 145  49 111  13  63  41 101  23  71  21  80 142 140 122\n",
      "  12  91  98  52  36 116 138 107  45 141  61 123  18  14  69 144   9  28\n",
      "  43   8 103  93  87  78 102  83  40 134  46  57  89  54 113   3  31  90\n",
      "  62  51  74 125  72  35  76 124  65  17  56  85 127 146  59 104  24 131\n",
      "  79  47 115 128  10  50  53  86 126  97 117 133   1  25  64  96  42 121\n",
      " 106  39  88  26  27  20  82  77   2  48  75   0 135  29   4  95 110 114\n",
      "   5  33   7 108 132  58  38  34 109  32  19 139  73 120  84  66 137  15\n",
      " 105  67 100 118  92 136 143  22 112  44  94  11  16  37 130  68  99 119\n",
      "   6  70  30  60]\n",
      "Embarked:Index(['C', 'Q', 'S'], dtype='object')\n",
      "Embarked编码后： [ 2  0  1 -1]\n",
      "特征范围： {'Age': [0.42, 9.262222222222222, 18.104444444444447, 26.94666666666667, 35.78888888888889, 44.63111111111111, 53.473333333333336, 62.31555555555556, 71.15777777777778, 80.0], 'Fare': [0.0, 56.925466666666665, 113.85093333333333, 170.7764, 227.70186666666666, 284.62733333333335, 341.5528, 398.4782666666666, 455.4037333333333, 512.3292], 'Sex': [0, 1], 'Pclass': [0, 1, 2], 'SibSp': [0, 1, 2, 3, 4, 5, 6], 'Parch': [0, 1, 2, 3, 4, 5, 6], 'Cabin': [-1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146], 'Embarked': [-1, 0, 1, 2]}\n"
     ]
    }
   ],
   "source": [
    "# 只有有限取值的离散特征\n",
    "cat_feat=['Sex','Pclass','SibSp','Parch','Cabin','Embarked']\n",
    "for feat in cat_feat:\n",
    "    data[feat]=data[feat].astype('category') # 转换为类别型数据\n",
    "    print(f'{feat}:{data[feat].cat.categories}') # 查看类别型数据的类别,.cat表示类别型数据的属性，.categories表示类别\n",
    "    data[feat]=data[feat].cat.codes.to_list() # 将类别按顺序转换为整数\n",
    "    print(f'{feat}编码后：', data[feat].unique()) # 查看编码后的唯一值\n",
    "    ranges=list(set(data[feat]))\n",
    "    ranges.sort()\n",
    "    feat_ranges[feat]=ranges # 存储离散特征的取值范围"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7149be14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将所有缺省值替换为-1\n",
    "data.fillna(-1, inplace=True)\n",
    "for feat in feat_ranges.keys():\n",
    "    feat_ranges[feat]=[-1] + feat_ranges[feat] # 在每个特征的范围前添加-1，表示缺省值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3f1ab7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集大小： 712\n",
      "测试集大小： 179\n",
      "特征数： 8\n"
     ]
    }
   ],
   "source": [
    "# 划分训练集与测试集\n",
    "np.random.seed(0)\n",
    "feat_names = data.columns[1:]\n",
    "label_name = data.columns[0]\n",
    "# 重排下标之后，按新的下标索引数据\n",
    "data = data.reindex(np.random.permutation(data.index))\n",
    "ratio = 0.8\n",
    "split = int(ratio * len(data))\n",
    "train_x = data[:split].drop(columns=['Survived']).to_numpy()\n",
    "train_y = data['Survived'][:split].to_numpy()\n",
    "test_x = data[split:].drop(columns=['Survived']).to_numpy()\n",
    "test_y = data['Survived'][split:].to_numpy()\n",
    "print('训练集大小：', len(train_x))\n",
    "print('测试集大小：', len(test_x))\n",
    "print('特征数：', train_x.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649c3df9",
   "metadata": {},
   "source": [
    "C4.5的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6174c5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "\n",
    "    def __init__(self):\n",
    "        # 内部节点的feat表示用来分类的特征编号，其数字与数据中的顺序对应\n",
    "        # 叶节点的feat表示该节点对应的分类结果\n",
    "        self.feat=None\n",
    "        # 分类值列表，表示按照其中的值向子节点分类\n",
    "        self.split=None\n",
    "        # 子节点列表，表示每个分类值对应的子节点\n",
    "        self.child=[]\n",
    "\n",
    "class DecisionTree:\n",
    "\n",
    "    def __init__(self, X, Y, feat_ranges, lbd):\n",
    "        self.root = Node()\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.feat_ranges = feat_ranges # 特征取值范围\n",
    "        self.lbd = lbd # 正则化系数\n",
    "        self.eps = 1e-8 # 防止数学错误log(0)和除以0\n",
    "        self.T = 0 # 记录叶节点个数\n",
    "        self.ID3(self.root, self.X, self.Y)\n",
    "\n",
    "    # 工具函数，计算 a * log a\n",
    "    def aloga(self, a):\n",
    "        return a * np.log2(a + self.eps)\n",
    "\n",
    "    # 计算某个子数据集的熵\n",
    "    def entropy(self, Y):\n",
    "        cnt = np.unique(Y, return_counts=True)[1] # 统计每个类别出现的次数\n",
    "        N = len(Y)\n",
    "        ent = -np.sum([self.aloga(Ni / N) for Ni in cnt])\n",
    "        return ent\n",
    "\n",
    "    # 计算用feat <= val划分数据集的信息增益\n",
    "    def info_gain(self, X, Y, feat, val):\n",
    "        # 划分前的熵\n",
    "        N = len(Y)\n",
    "        if N == 0:\n",
    "            return 0\n",
    "        HX = self.entropy(Y)\n",
    "        HXY = 0 # H(X|Y)\n",
    "        # 分别计算H(X|X_F<=val)和H(X|X_F>val)\n",
    "        Y_l = Y[X[:, feat] <= val]\n",
    "        HXY += len(Y_l) / len(Y) * self.entropy(Y_l)\n",
    "        Y_r = Y[X[:, feat] > val]\n",
    "        HXY += len(Y_r) / len(Y) * self.entropy(Y_r)\n",
    "        return HX - HXY\n",
    "\n",
    "    # 计算特征feat <= val本身的复杂度H_Y(X)\n",
    "    def entropy_YX(self, X, Y, feat, val):\n",
    "        HYX = 0\n",
    "        N = len(Y)\n",
    "        if N == 0:\n",
    "            return 0\n",
    "        Y_l = Y[X[:, feat] <= val]\n",
    "        HYX += -self.aloga(len(Y_l) / N)\n",
    "        Y_r = Y[X[:, feat] > val]\n",
    "        HYX += -self.aloga(len(Y_r) / N)\n",
    "        return HYX\n",
    "\n",
    "    # 计算用feat <= val划分数据集的信息增益率\n",
    "    def info_gain_ratio(self, X, Y, feat, val):\n",
    "        IG = self.info_gain(X, Y, feat, val)\n",
    "        HYX = self.entropy_YX(X, Y, feat, val)\n",
    "        return IG / HYX\n",
    "\n",
    "    # 用ID3算法递归分裂节点，构造决策树\n",
    "    def ID3(self, node, X, Y):\n",
    "        # 判断是否已经分类完成\n",
    "        if len(np.unique(Y)) == 1:\n",
    "            node.feat = Y[0]\n",
    "            self.T += 1\n",
    "            return\n",
    "        \n",
    "        # 寻找最优分类特征和分类点\n",
    "        best_IGR = 0 # 最优信息增益率\n",
    "        best_feat = None # 最优特征编号\n",
    "        best_val = None # 最优特征取值\n",
    "        for feat in range(len(feat_names)):\n",
    "            for val in self.feat_ranges[feat_names[feat]]:\n",
    "                IGR = self.info_gain_ratio(X, Y, feat, val)\n",
    "                if IGR > best_IGR:\n",
    "                    best_IGR = IGR\n",
    "                    best_feat = feat\n",
    "                    best_val = val\n",
    "        \n",
    "        # 计算用best_feat <= best_val分类带来的代价函数变化\n",
    "        # 由于分裂叶节点只涉及该局部，我们只需要计算分裂前后该节点的代价函数\n",
    "        # 当前代价\n",
    "        cur_cost = len(Y) * self.entropy(Y) + self.lbd\n",
    "        # 分裂后的代价，按best_feat的取值分类统计\n",
    "        # 如果best_feat为None，说明最优的信息增益率为0，\n",
    "        # 再分类也无法增加信息了，因此将new_cost设置为无穷大\n",
    "        if best_feat is None:\n",
    "            new_cost = np.inf\n",
    "        else:\n",
    "            new_cost = 0\n",
    "            X_feat = X[:, best_feat] \n",
    "            # 获取划分后的两部分，计算新的熵\n",
    "            new_Y_l = Y[X_feat <= best_val]\n",
    "            new_cost += len(new_Y_l) * self.entropy(new_Y_l)\n",
    "            new_Y_r = Y[X_feat > best_val]\n",
    "            new_cost += len(new_Y_r) * self.entropy(new_Y_r)\n",
    "            # 分裂后会有两个叶节点\n",
    "            new_cost += 2 * self.lbd\n",
    "\n",
    "        if new_cost <= cur_cost:\n",
    "            # 如果分裂后代价更小，那么执行分裂\n",
    "            node.feat = best_feat\n",
    "            node.split = best_val\n",
    "            l_child = Node()\n",
    "            l_X = X[X_feat <= best_val]\n",
    "            l_Y = Y[X_feat <= best_val]\n",
    "            self.ID3(l_child, l_X, l_Y)\n",
    "            r_child = Node()\n",
    "            r_X = X[X_feat > best_val]\n",
    "            r_Y = Y[X_feat > best_val]\n",
    "            self.ID3(r_child, r_X, r_Y)\n",
    "            node.child = [l_child, r_child]\n",
    "        else:\n",
    "            # 否则将当前节点上最多的类别作为该节点的类别\n",
    "            vals, cnt = np.unique(Y, return_counts=True)\n",
    "            node.feat = vals[np.argmax(cnt)]\n",
    "            self.T += 1\n",
    "\n",
    "    # 预测新样本的分类\n",
    "    def predict(self, x):\n",
    "        node = self.root\n",
    "        # 从根节点开始向下寻找，到叶节点结束\n",
    "        while node.split is not None:\n",
    "            # 判断x应该处于哪个子节点\n",
    "            if x[node.feat] <= node.split:\n",
    "                node = node.child[0]\n",
    "            else:\n",
    "                node = node.child[1]\n",
    "        # 到达叶节点，返回类别\n",
    "        return node.feat\n",
    "\n",
    "    # 计算在样本X，标签Y上的准确率\n",
    "    def accuracy(self, X, Y):\n",
    "        correct = 0\n",
    "        for x, y in zip(X, Y):\n",
    "            pred = self.predict(x)\n",
    "            if pred == y:\n",
    "                correct += 1\n",
    "        return correct / len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f2035479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "λ=0.01, 训练集准确率=0.9199, 测试集准确率=0.75418994\n",
      "λ=0.26, 训练集准确率=0.9185, 测试集准确率=0.76536313\n",
      "λ=0.51, 训练集准确率=0.9143, 测试集准确率=0.76536313\n",
      "λ=0.75, 训练集准确率=0.9101, 测试集准确率=0.76536313\n",
      "λ=1.00, 训练集准确率=0.8301, 测试集准确率=0.72625698\n",
      "(np.float64(0.2575), (0.9185393258426966, 0.7653631284916201))\n",
      "最大测试集准确率=0.7654，对应的λ=0.26\n"
     ]
    }
   ],
   "source": [
    "result={}\n",
    "for lbd in np.linspace(0.01, 1, 5): \n",
    "    # 创建决策树对象\n",
    "    DT = DecisionTree(train_x, train_y, feat_ranges, lbd)\n",
    "    # 计算在训练集和测试集上的准确率\n",
    "    train_acc = DT.accuracy(train_x, train_y)\n",
    "    test_acc = DT.accuracy(test_x, test_y)\n",
    "    result[lbd] = (train_acc, test_acc)\n",
    "    # print(result)\n",
    "    print(f'λ={lbd:.2f}, 训练集准确率={train_acc:.4f}, 测试集准确率={test_acc:.8f}')\n",
    "\n",
    "# 输出最大的测试集准确率和对应的λ\n",
    "max_test_acc = max(result.items(), key=lambda x: x[1][1])\n",
    "print(max_test_acc)\n",
    "print(f'最大测试集准确率={max_test_acc[1][1]:.4f}，对应的λ={max_test_acc[0]:.2f}')\n",
    "\n",
    "\n",
    "# DT = DecisionTree(train_x, train_y, feat_ranges, lbd=0.005)\n",
    "# print('叶节点数量：', DT.T)\n",
    "\n",
    "# # 计算在训练集和测试集上的准确率\n",
    "# print('训练集准确率：', DT.accuracy(train_x, train_y))\n",
    "# print('测试集准确率：', DT.accuracy(test_x, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2501a195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集准确率：C4.5：0.8567415730337079，CART：0.8623595505617978\n",
      "测试集准确率：C4.5：0.8324022346368715，CART：0.8379888268156425\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "# criterion表示分类依据，max_depth表示树的最大深度\n",
    "# entropy生成的是C4.5分类树\n",
    "c45 = tree.DecisionTreeClassifier(criterion='entropy', max_depth=5)\n",
    "c45.fit(train_x, train_y)\n",
    "# gini生成的是CART分类树\n",
    "cart = tree.DecisionTreeClassifier(criterion='gini', max_depth=5)\n",
    "cart.fit(train_x, train_y)\n",
    "\n",
    "c45_train_pred = c45.predict(train_x)\n",
    "c45_test_pred = c45.predict(test_x)\n",
    "cart_train_pred = cart.predict(train_x)\n",
    "cart_test_pred = cart.predict(test_x)\n",
    "print(f'训练集准确率：C4.5：{np.mean(c45_train_pred == train_y)}，' \\\n",
    "    f'CART：{np.mean(cart_train_pred == train_y)}')\n",
    "print(f'测试集准确率：C4.5：{np.mean(c45_test_pred == test_y)}，' \\\n",
    "    f'CART：{np.mean(cart_test_pred == test_y)}')\n",
    "\n",
    "from six import StringIO\n",
    "import pydotplus\n",
    "\n",
    "dot_data = StringIO()\n",
    "tree.export_graphviz( # 导出sklearn的决策树的可视化数据\n",
    "    c45,\n",
    "    out_file=dot_data,\n",
    "    feature_names=feat_names,\n",
    "    class_names=['non-survival', 'survival'],\n",
    "    filled=True, \n",
    "    rounded=True,\n",
    "    impurity=False\n",
    ")\n",
    "# 用pydotplus生成图像\n",
    "graph = pydotplus.graph_from_dot_data(\n",
    "    dot_data.getvalue().replace('\\n', '')) \n",
    "graph.write_png('tree.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e08e48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
